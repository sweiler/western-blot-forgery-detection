//
// Generated by NVIDIA NVVM Compiler
// Compiler built on Thu Jul 18 02:37:37 2013 (1374107857)
// Cuda compilation tools, release 5.5, V5.5.0
//

.version 3.2
.target sm_30
.address_size 64

	.file	1 "/home/ralf/workspace/forgery_detector/forgery_detector/ZernikeCUDA/Release/../zernikepolynomial.cu", 1383117954, 5114
	.file	2 "/usr/local/cuda-5.5/bin/../targets/x86_64-linux/include/device_functions.h", 1374240224, 185228
	.file	3 "/usr/local/cuda-5.5/bin/../targets/x86_64-linux/include/math_functions_dbl_ptx3.h", 1374240224, 118830
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.const .align 4 .b8 factorials[24] = {1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 6, 0, 0, 0, 24, 0, 0, 0, 120, 0, 0, 0};
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .func  (.param .b64 func_retval0) _Z19getAverageIntensityiPhii(
	.param .b32 _Z19getAverageIntensityiPhii_param_0,
	.param .b64 _Z19getAverageIntensityiPhii_param_1,
	.param .b32 _Z19getAverageIntensityiPhii_param_2,
	.param .b32 _Z19getAverageIntensityiPhii_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .s32 	%r<17>;
	.reg .s64 	%rd<4>;
	.reg .f64 	%fd<11>;


	ld.param.u32 	%r6, [_Z19getAverageIntensityiPhii_param_0];
	ld.param.u64 	%rd1, [_Z19getAverageIntensityiPhii_param_1];
	ld.param.u32 	%r7, [_Z19getAverageIntensityiPhii_param_2];
	ld.param.u32 	%r8, [_Z19getAverageIntensityiPhii_param_3];
	.loc 1 20 1
	setp.gt.s32	%p1, %r6, 0;
	@%p1 bra 	BB0_2;

	mov.f64 	%fd10, 0d0000000000000000;
	bra.uni 	BB0_6;

BB0_2:
	mov.u32 	%r9, 0;
	mov.f64 	%fd10, 0d0000000000000000;
	mov.u32 	%r16, %r9;

BB0_3:
	.loc 1 22 1
	add.s32 	%r2, %r16, %r7;
	mov.u32 	%r15, %r9;

BB0_4:
	.loc 1 22 1
	mov.u32 	%r3, %r15;
	mad.lo.s32 	%r11, %r3, %r8, %r2;
	cvt.s64.s32	%rd2, %r11;
	add.s64 	%rd3, %rd1, %rd2;
	.loc 1 22 1
	ld.u8 	%r12, [%rd3];
	cvt.rn.f64.s32	%fd7, %r12;
	add.f64 	%fd10, %fd10, %fd7;
	.loc 1 21 101
	add.s32 	%r4, %r3, 1;
	.loc 1 21 1
	setp.lt.s32	%p2, %r4, %r6;
	mov.u32 	%r15, %r4;
	@%p2 bra 	BB0_4;

	.loc 1 20 101
	add.s32 	%r16, %r16, 1;
	.loc 1 20 1
	setp.lt.s32	%p3, %r16, %r6;
	@%p3 bra 	BB0_3;

BB0_6:
	.loc 1 25 1
	mul.lo.s32 	%r13, %r6, %r6;
	cvt.rn.f64.s32	%fd8, %r13;
	.loc 2 3614 3
	div.rn.f64 	%fd9, %fd10, %fd8;
	st.param.f64	[func_retval0+0], %fd9;
	.loc 1 25 17
	ret;
}

.visible .func  (.param .b32 func_retval0) _Z8getRangeiPhii(
	.param .b32 _Z8getRangeiPhii_param_0,
	.param .b64 _Z8getRangeiPhii_param_1,
	.param .b32 _Z8getRangeiPhii_param_2,
	.param .b32 _Z8getRangeiPhii_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .s32 	%r<36>;
	.reg .s64 	%rd<4>;


	ld.param.u32 	%r14, [_Z8getRangeiPhii_param_0];
	ld.param.u64 	%rd1, [_Z8getRangeiPhii_param_1];
	ld.param.u32 	%r15, [_Z8getRangeiPhii_param_2];
	ld.param.u32 	%r16, [_Z8getRangeiPhii_param_3];
	mov.u32 	%r35, 255;
	.loc 1 34 1
	setp.gt.s32	%p1, %r14, 0;
	@%p1 bra 	BB1_2;

	mov.u32 	%r31, 0;
	bra.uni 	BB1_6;

BB1_2:
	mov.u32 	%r20, 0;
	mov.u32 	%r26, %r20;
	mov.u32 	%r34, %r20;

BB1_3:
	.loc 1 36 1
	mov.u32 	%r30, %r34;
	mov.u32 	%r32, %r30;
	add.s32 	%r4, %r26, %r15;
	mov.u32 	%r33, %r20;

BB1_4:
	.loc 1 36 1
	mov.u32 	%r5, %r33;
	mad.lo.s32 	%r23, %r5, %r16, %r4;
	cvt.s64.s32	%rd2, %r23;
	add.s64 	%rd3, %rd1, %rd2;
	.loc 1 36 1
	ld.u8 	%r24, [%rd3];
	max.s32 	%r32, %r24, %r32;
	min.s32 	%r35, %r24, %r35;
	.loc 1 35 101
	add.s32 	%r10, %r5, 1;
	.loc 1 35 1
	setp.lt.s32	%p2, %r10, %r14;
	mov.u32 	%r33, %r10;
	@%p2 bra 	BB1_4;

	.loc 1 34 101
	add.s32 	%r26, %r26, 1;
	.loc 1 34 1
	setp.lt.s32	%p3, %r26, %r14;
	mov.u32 	%r31, %r32;
	mov.u32 	%r34, %r32;
	@%p3 bra 	BB1_3;

BB1_6:
	.loc 1 44 1
	sub.s32 	%r25, %r31, %r35;
	st.param.b32	[func_retval0+0], %r25;
	ret;
}

.visible .func  (.param .b64 func_retval0) _Z19getRhoFromCartesiandd(
	.param .b64 _Z19getRhoFromCartesiandd_param_0,
	.param .b64 _Z19getRhoFromCartesiandd_param_1
)
{
	.reg .f64 	%fd<6>;


	ld.param.f64 	%fd1, [_Z19getRhoFromCartesiandd_param_0];
	ld.param.f64 	%fd2, [_Z19getRhoFromCartesiandd_param_1];
	.loc 1 48 1
	mul.f64 	%fd3, %fd2, %fd2;
	fma.rn.f64 	%fd4, %fd1, %fd1, %fd3;
	.loc 2 3060 10
	sqrt.rn.f64 	%fd5, %fd4;
	st.param.f64	[func_retval0+0], %fd5;
	.loc 1 48 8
	ret;
}

.visible .func  (.param .b64 func_retval0) _Z21getThetaFromCartesiandd(
	.param .b64 _Z21getThetaFromCartesiandd_param_0,
	.param .b64 _Z21getThetaFromCartesiandd_param_1
)
{
	.reg .pred 	%p<15>;
	.reg .s32 	%r<13>;
	.reg .f64 	%fd<64>;


	ld.param.f64 	%fd7, [_Z21getThetaFromCartesiandd_param_0];
	ld.param.f64 	%fd8, [_Z21getThetaFromCartesiandd_param_1];
	.loc 1 52 1
	setp.eq.f64	%p1, %fd7, 0d0000000000000000;
	setp.eq.f64	%p2, %fd8, 0d0000000000000000;
	and.pred  	%p3, %p1, %p2;
	.loc 1 52 1
	@!%p3 bra 	BB3_2;
	bra.uni 	BB3_1;

BB3_1:
	mov.f64 	%fd63, 0d0000000000000000;
	bra.uni 	BB3_7;

BB3_2:
	.loc 3 278 10
	abs.f64 	%fd1, %fd8;
	setp.eq.f64	%p4, %fd1, 0d0000000000000000;
	abs.f64 	%fd2, %fd7;
	setp.eq.f64	%p5, %fd2, 0d0000000000000000;
	and.pred  	%p6, %p4, %p5;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd8;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd7;
	}
	and.b32  	%r2, %r3, -2147483648;
	@%p6 bra 	BB3_6;

	setp.eq.f64	%p7, %fd1, 0d7FF0000000000000;
	setp.eq.f64	%p8, %fd2, 0d7FF0000000000000;
	and.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB3_5;

	setp.lt.s32	%p10, %r1, 0;
	.loc 3 278 10
	min.f64 	%fd9, %fd2, %fd1;
	max.f64 	%fd10, %fd2, %fd1;
	div.rn.f64 	%fd11, %fd9, %fd10;
	mul.f64 	%fd12, %fd11, %fd11;
	mov.f64 	%fd13, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd14, 0dBEF53E1D2A25FF7E;
	.loc 3 278 10
	fma.rn.f64 	%fd15, %fd14, %fd12, %fd13;
	mov.f64 	%fd16, 0dBF5312788DDE082E;
	.loc 3 278 10
	fma.rn.f64 	%fd17, %fd15, %fd12, %fd16;
	mov.f64 	%fd18, 0d3F6F9690C8249315;
	.loc 3 278 10
	fma.rn.f64 	%fd19, %fd17, %fd12, %fd18;
	mov.f64 	%fd20, 0dBF82CF5AABC7CF0D;
	.loc 3 278 10
	fma.rn.f64 	%fd21, %fd19, %fd12, %fd20;
	mov.f64 	%fd22, 0d3F9162B0B2A3BFDE;
	.loc 3 278 10
	fma.rn.f64 	%fd23, %fd21, %fd12, %fd22;
	mov.f64 	%fd24, 0dBF9A7256FEB6FC6B;
	.loc 3 278 10
	fma.rn.f64 	%fd25, %fd23, %fd12, %fd24;
	mov.f64 	%fd26, 0d3FA171560CE4A489;
	.loc 3 278 10
	fma.rn.f64 	%fd27, %fd25, %fd12, %fd26;
	mov.f64 	%fd28, 0dBFA4F44D841450E4;
	.loc 3 278 10
	fma.rn.f64 	%fd29, %fd27, %fd12, %fd28;
	mov.f64 	%fd30, 0d3FA7EE3D3F36BB95;
	.loc 3 278 10
	fma.rn.f64 	%fd31, %fd29, %fd12, %fd30;
	mov.f64 	%fd32, 0dBFAAD32AE04A9FD1;
	.loc 3 278 10
	fma.rn.f64 	%fd33, %fd31, %fd12, %fd32;
	mov.f64 	%fd34, 0d3FAE17813D66954F;
	.loc 3 278 10
	fma.rn.f64 	%fd35, %fd33, %fd12, %fd34;
	mov.f64 	%fd36, 0dBFB11089CA9A5BCD;
	.loc 3 278 10
	fma.rn.f64 	%fd37, %fd35, %fd12, %fd36;
	mov.f64 	%fd38, 0d3FB3B12B2DB51738;
	.loc 3 278 10
	fma.rn.f64 	%fd39, %fd37, %fd12, %fd38;
	mov.f64 	%fd40, 0dBFB745D022F8DC5C;
	.loc 3 278 10
	fma.rn.f64 	%fd41, %fd39, %fd12, %fd40;
	mov.f64 	%fd42, 0d3FBC71C709DFE927;
	.loc 3 278 10
	fma.rn.f64 	%fd43, %fd41, %fd12, %fd42;
	mov.f64 	%fd44, 0dBFC2492491FA1744;
	.loc 3 278 10
	fma.rn.f64 	%fd45, %fd43, %fd12, %fd44;
	mov.f64 	%fd46, 0d3FC99999999840D2;
	.loc 3 278 10
	fma.rn.f64 	%fd47, %fd45, %fd12, %fd46;
	mov.f64 	%fd48, 0dBFD555555555544C;
	.loc 3 278 10
	fma.rn.f64 	%fd49, %fd47, %fd12, %fd48;
	mul.f64 	%fd50, %fd49, %fd12;
	fma.rn.f64 	%fd51, %fd50, %fd11, %fd11;
	mov.f64 	%fd52, 0d3FF921FB54442D18;
	.loc 3 278 10
	sub.f64 	%fd53, %fd52, %fd51;
	setp.gt.f64	%p11, %fd2, %fd1;
	selp.f64	%fd54, %fd53, %fd51, %p11;
	mov.f64 	%fd55, 0d400921FB54442D18;
	.loc 3 278 10
	sub.f64 	%fd56, %fd55, %fd54;
	selp.f64	%fd57, %fd56, %fd54, %p10;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r4, %temp}, %fd57;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r5}, %fd57;
	}
	or.b32  	%r6, %r5, %r2;
	mov.b64 	%fd58, {%r4, %r6};
	add.f64 	%fd59, %fd1, %fd2;
	setp.gtu.f64	%p12, %fd59, 0d7FF0000000000000;
	selp.f64	%fd63, %fd59, %fd58, %p12;
	bra.uni 	BB3_7;

BB3_5:
	setp.lt.s32	%p13, %r1, 0;
	.loc 3 278 10
	selp.f64	%fd60, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p13;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r7, %temp}, %fd60;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd60;
	}
	or.b32  	%r9, %r8, %r2;
	mov.b64 	%fd63, {%r7, %r9};
	bra.uni 	BB3_7;

BB3_6:
	setp.lt.s32	%p14, %r1, 0;
	.loc 3 278 10
	selp.f64	%fd61, 0d400921FB54442D18, 0d0000000000000000, %p14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r10, %temp}, %fd61;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r11}, %fd61;
	}
	or.b32  	%r12, %r11, %r2;
	mov.b64 	%fd63, {%r10, %r12};

BB3_7:
	st.param.f64	[func_retval0+0], %fd63;
	.loc 1 55 8
	ret;
}

.visible .func  (.param .b64 func_retval0) _Z26getZernikeRadialPolynomialiid(
	.param .b32 _Z26getZernikeRadialPolynomialiid_param_0,
	.param .b32 _Z26getZernikeRadialPolynomialiid_param_1,
	.param .b64 _Z26getZernikeRadialPolynomialiid_param_2
)
{
	.reg .pred 	%p<38>;
	.reg .s32 	%r<59>;
	.reg .s64 	%rd<20>;
	.reg .f64 	%fd<55>;


	ld.param.u32 	%r57, [_Z26getZernikeRadialPolynomialiid_param_0];
	ld.param.u32 	%r13, [_Z26getZernikeRadialPolynomialiid_param_1];
	ld.param.f64 	%fd27, [_Z26getZernikeRadialPolynomialiid_param_2];
	.loc 1 61 1
	sub.s32 	%r1, %r57, %r13;
	and.b32  	%r14, %r1, 1;
	setp.eq.b32	%p1, %r14, 1;
	.loc 1 80 1
	mov.f64 	%fd54, 0d0000000000000000;
	.loc 1 61 1
	@!%p1 bra 	BB4_1;
	bra.uni 	BB4_34;

BB4_1:
	.loc 1 65 1
	shr.u32 	%r15, %r1, 31;
	add.s32 	%r16, %r1, %r15;
	shr.s32 	%r2, %r16, 1;
	setp.gt.s32	%p2, %r1, -2;
	@%p2 bra 	BB4_2;
	bra.uni 	BB4_34;

BB4_2:
	mov.f64 	%fd31, 0dBFF0000000000000;
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd31;
	}
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd27;
	}
	.loc 1 69 200
	add.s32 	%r18, %r13, %r57;
	shr.u32 	%r19, %r18, 31;
	add.s32 	%r20, %r18, %r19;
	shr.s32 	%r21, %r20, 1;
	mul.wide.s32 	%rd13, %r57, 4;
	mov.u64 	%rd16, factorials;
	add.s64 	%rd19, %rd16, %rd13;
	.loc 1 65 1
	mul.wide.s32 	%rd14, %r21, 4;
	add.s64 	%rd18, %rd16, %rd14;
	mul.wide.s32 	%rd15, %r2, 4;
	add.s64 	%rd17, %rd16, %rd15;
	mov.f64 	%fd54, 0d0000000000000000;
	mov.u32 	%r58, 0;
	mov.f64 	%fd51, %fd54;

BB4_3:
	neg.s32 	%r7, %r57;
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd51;
	}
	mul.f64 	%fd32, %fd51, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd33, %fd32;
	fma.rn.f64 	%fd34, %fd33, 0dC000000000000000, %fd51;
	abs.f64 	%fd3, %fd34;
	setp.ne.s32	%p3, %r58, 0;
	@%p3 bra 	BB4_5;

	mov.f64 	%fd52, 0d3FF0000000000000;
	bra.uni 	BB4_17;

BB4_5:
	.loc 3 328 10
	abs.f64 	%fd4, %fd31;
	setp.gtu.f64	%p4, %fd4, 0d7FF0000000000000;
	@%p4 bra 	BB4_16;

	abs.f64 	%fd5, %fd51;
	setp.gtu.f64	%p5, %fd5, 0d7FF0000000000000;
	@%p5 bra 	BB4_16;

	setp.eq.f64	%p6, %fd5, 0d7FF0000000000000;
	@%p6 bra 	BB4_15;

	setp.eq.f64	%p7, %fd4, 0d7FF0000000000000;
	@%p7 bra 	BB4_14;

	setp.gt.s32	%p8, %r3, -1;
	@%p8 bra 	BB4_12;

	cvt.rzi.f64.f64	%fd36, %fd51;
	setp.eq.f64	%p9, %fd51, %fd36;
	@%p9 bra 	BB4_12;

	mov.f64 	%fd52, 0dFFF8000000000000;
	bra.uni 	BB4_17;

BB4_12:
	setp.eq.f64	%p10, %fd3, 0d3FF0000000000000;
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd4;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd51;
	.param .b64 retval0;
	.loc 3 328 10
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd52, [retval0+0];
	}
	// Callseq End 0
	setp.lt.s32	%p11, %r3, 0;
	.loc 3 328 10
	and.pred  	%p12, %p11, %p10;
	@!%p12 bra 	BB4_17;
	bra.uni 	BB4_13;

BB4_13:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r22}, %fd52;
	}
	xor.b32  	%r23, %r22, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r24, %temp}, %fd52;
	}
	mov.b64 	%fd52, {%r24, %r23};
	bra.uni 	BB4_17;

BB4_14:
	setp.eq.f64	%p13, %fd3, 0d3FF0000000000000;
	.loc 3 328 10
	shr.s32 	%r25, %r8, 31;
	and.b32  	%r26, %r25, -2146435072;
	add.s32 	%r27, %r26, 2146435072;
	setp.lt.s32	%p14, %r3, 0;
	mov.u32 	%r28, 0;
	.loc 3 328 10
	and.pred  	%p15, %p14, %p13;
	or.b32  	%r29, %r27, -2147483648;
	selp.b32	%r30, %r29, %r27, %p15;
	mov.b64 	%fd52, {%r28, %r30};
	bra.uni 	BB4_17;

BB4_15:
	mov.u32 	%r31, 1072693248;
	mov.u32 	%r32, 0;
	.loc 3 328 10
	mov.b64 	%fd52, {%r32, %r31};
	bra.uni 	BB4_17;

BB4_16:
	.loc 3 328 10
	add.f64 	%fd52, %fd51, 0dBFF0000000000000;

BB4_17:
	.loc 1 69 64
	ld.const.u32 	%r33, [%rd19];
	cvt.rn.f64.s32	%fd39, %r33;
	mul.f64 	%fd12, %fd52, %fd39;
	cvt.rn.f64.s32	%fd13, %r57;
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd13;
	}
	mul.f64 	%fd40, %fd13, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd41, %fd40;
	fma.rn.f64 	%fd42, %fd41, 0dC000000000000000, %fd13;
	abs.f64 	%fd14, %fd42;
	setp.eq.s32	%p16, %r7, 0;
	setp.eq.f64	%p17, %fd27, 0d3FF0000000000000;
	.loc 3 328 10
	or.pred  	%p18, %p17, %p16;
	@!%p18 bra 	BB4_19;
	bra.uni 	BB4_18;

BB4_18:
	mov.f64 	%fd53, 0d3FF0000000000000;
	bra.uni 	BB4_33;

BB4_19:
	.loc 3 328 10
	abs.f64 	%fd15, %fd27;
	setp.gtu.f64	%p19, %fd15, 0d7FF0000000000000;
	@%p19 bra 	BB4_32;

	abs.f64 	%fd16, %fd13;
	setp.gtu.f64	%p20, %fd16, 0d7FF0000000000000;
	@%p20 bra 	BB4_32;

	setp.eq.f64	%p21, %fd16, 0d7FF0000000000000;
	@%p21 bra 	BB4_31;

	setp.eq.f64	%p22, %fd15, 0d7FF0000000000000;
	@%p22 bra 	BB4_30;

	setp.eq.f64	%p23, %fd27, 0d0000000000000000;
	.loc 3 328 10
	@%p23 bra 	BB4_29;

	setp.gt.s32	%p24, %r4, -1;
	@%p24 bra 	BB4_27;

	cvt.rzi.f64.f64	%fd43, %fd13;
	setp.eq.f64	%p25, %fd13, %fd43;
	@%p25 bra 	BB4_27;

	mov.f64 	%fd53, 0dFFF8000000000000;
	bra.uni 	BB4_33;

BB4_27:
	setp.eq.f64	%p26, %fd14, 0d3FF0000000000000;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd15;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd13;
	.param .b64 retval0;
	.loc 3 328 10
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd53, [retval0+0];
	}
	// Callseq End 1
	setp.lt.s32	%p27, %r4, 0;
	.loc 3 328 10
	and.pred  	%p28, %p27, %p26;
	@!%p28 bra 	BB4_33;
	bra.uni 	BB4_28;

BB4_28:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r34}, %fd53;
	}
	xor.b32  	%r35, %r34, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r36, %temp}, %fd53;
	}
	mov.b64 	%fd53, {%r36, %r35};
	bra.uni 	BB4_33;

BB4_29:
	setp.eq.f64	%p29, %fd14, 0d3FF0000000000000;
	.loc 3 328 10
	selp.b32	%r37, %r4, 0, %p29;
	mov.u32 	%r38, 0;
	.loc 3 328 10
	or.b32  	%r39, %r37, 2146435072;
	setp.lt.s32	%p30, %r9, 0;
	selp.b32	%r40, %r39, %r37, %p30;
	mov.b64 	%fd53, {%r38, %r40};
	bra.uni 	BB4_33;

BB4_30:
	setp.eq.f64	%p31, %fd14, 0d3FF0000000000000;
	.loc 3 328 10
	shr.s32 	%r41, %r9, 31;
	and.b32  	%r42, %r41, -2146435072;
	add.s32 	%r43, %r42, 2146435072;
	setp.lt.s32	%p32, %r4, 0;
	mov.u32 	%r44, 0;
	.loc 3 328 10
	and.pred  	%p33, %p32, %p31;
	or.b32  	%r45, %r43, -2147483648;
	selp.b32	%r46, %r45, %r43, %p33;
	mov.b64 	%fd53, {%r44, %r46};
	bra.uni 	BB4_33;

BB4_31:
	setp.eq.f64	%p34, %fd27, 0dBFF0000000000000;
	.loc 3 328 10
	setp.gt.f64	%p35, %fd15, 0d3FF0000000000000;
	selp.b32	%r47, 2146435072, 0, %p35;
	mov.u32 	%r48, 0;
	.loc 3 328 10
	xor.b32  	%r49, %r47, 2146435072;
	setp.lt.s32	%p36, %r9, 0;
	selp.b32	%r50, %r49, %r47, %p36;
	selp.b32	%r51, 1072693248, %r50, %p34;
	mov.b64 	%fd53, {%r48, %r51};
	bra.uni 	BB4_33;

BB4_32:
	.loc 3 328 10
	add.f64 	%fd53, %fd13, %fd27;

BB4_33:
	.loc 1 69 200
	ld.const.u32 	%r52, [%rd18];
	ld.const.u32 	%r53, [%rd16];
	mul.lo.s32 	%r54, %r52, %r53;
	ld.const.u32 	%r55, [%rd17];
	mul.lo.s32 	%r56, %r54, %r55;
	cvt.rn.f64.s32	%fd46, %r56;
	mul.f64 	%fd47, %fd12, %fd53;
	.loc 2 3614 3
	div.rn.f64 	%fd48, %fd47, %fd46;
	.loc 1 78 1
	add.f64 	%fd54, %fd54, %fd48;
	add.f64 	%fd51, %fd51, 0d3FF0000000000000;
	.loc 1 65 1
	add.s64 	%rd19, %rd19, -4;
	add.s64 	%rd18, %rd18, -4;
	add.s64 	%rd17, %rd17, -4;
	add.s32 	%r57, %r57, -2;
	add.s64 	%rd16, %rd16, 4;
	.loc 1 65 72
	add.s32 	%r58, %r58, 1;
	.loc 1 65 1
	setp.le.s32	%p37, %r58, %r2;
	@%p37 bra 	BB4_3;

BB4_34:
	st.param.f64	[func_retval0+0], %fd54;
	.loc 1 80 1
	ret;
}

.visible .func  (.param .align 16 .b8 func_retval0[16]) _Z16getZernikeMomentiidd(
	.param .b32 _Z16getZernikeMomentiidd_param_0,
	.param .b32 _Z16getZernikeMomentiidd_param_1,
	.param .b64 _Z16getZernikeMomentiidd_param_2,
	.param .b64 _Z16getZernikeMomentiidd_param_3
)
{
	.local .align 4 .b8 	__local_depot5[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<48>;
	.reg .s32 	%r<76>;
	.reg .s64 	%rd<34>;
	.reg .f64 	%fd<143>;


	mov.u64 	%SPL, __local_depot5;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r72, [_Z16getZernikeMomentiidd_param_0];
	ld.param.u32 	%r20, [_Z16getZernikeMomentiidd_param_1];
	ld.param.f64 	%fd55, [_Z16getZernikeMomentiidd_param_2];
	ld.param.f64 	%fd56, [_Z16getZernikeMomentiidd_param_3];
	.loc 1 61 1
	sub.s32 	%r1, %r72, %r20;
	and.b32  	%r21, %r1, 1;
	setp.eq.b32	%p1, %r21, 1;
	.loc 1 80 1
	mov.f64 	%fd135, 0d0000000000000000;
	.loc 1 61 1
	@!%p1 bra 	BB5_1;
	bra.uni 	BB5_34;

BB5_1:
	.loc 1 65 1
	shr.u32 	%r22, %r1, 31;
	add.s32 	%r23, %r1, %r22;
	shr.s32 	%r2, %r23, 1;
	setp.gt.s32	%p2, %r1, -2;
	@%p2 bra 	BB5_2;
	bra.uni 	BB5_34;

BB5_2:
	mov.f64 	%fd60, 0dBFF0000000000000;
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r3}, %fd60;
	}
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd55;
	}
	.loc 1 69 200
	add.s32 	%r25, %r20, %r72;
	shr.u32 	%r26, %r25, 31;
	add.s32 	%r27, %r25, %r26;
	shr.s32 	%r28, %r27, 1;
	mul.wide.s32 	%rd13, %r72, 4;
	mov.u64 	%rd30, factorials;
	add.s64 	%rd33, %rd30, %rd13;
	.loc 1 65 1
	mul.wide.s32 	%rd14, %r28, 4;
	add.s64 	%rd32, %rd30, %rd14;
	mul.wide.s32 	%rd15, %r2, 4;
	add.s64 	%rd31, %rd30, %rd15;
	mov.f64 	%fd135, 0d0000000000000000;
	mov.u32 	%r73, 0;
	mov.f64 	%fd132, %fd135;

BB5_3:
	neg.s32 	%r7, %r72;
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r8}, %fd132;
	}
	mul.f64 	%fd61, %fd132, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd62, %fd61;
	fma.rn.f64 	%fd63, %fd62, 0dC000000000000000, %fd132;
	abs.f64 	%fd3, %fd63;
	setp.ne.s32	%p3, %r73, 0;
	@%p3 bra 	BB5_5;

	mov.f64 	%fd133, 0d3FF0000000000000;
	bra.uni 	BB5_17;

BB5_5:
	.loc 3 328 10
	abs.f64 	%fd4, %fd60;
	setp.gtu.f64	%p4, %fd4, 0d7FF0000000000000;
	@%p4 bra 	BB5_16;

	abs.f64 	%fd5, %fd132;
	setp.gtu.f64	%p5, %fd5, 0d7FF0000000000000;
	@%p5 bra 	BB5_16;

	setp.eq.f64	%p6, %fd5, 0d7FF0000000000000;
	@%p6 bra 	BB5_15;

	setp.eq.f64	%p7, %fd4, 0d7FF0000000000000;
	@%p7 bra 	BB5_14;

	setp.gt.s32	%p8, %r3, -1;
	@%p8 bra 	BB5_12;

	cvt.rzi.f64.f64	%fd65, %fd132;
	setp.eq.f64	%p9, %fd132, %fd65;
	@%p9 bra 	BB5_12;

	mov.f64 	%fd133, 0dFFF8000000000000;
	bra.uni 	BB5_17;

BB5_12:
	setp.eq.f64	%p10, %fd3, 0d3FF0000000000000;
	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd4;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd132;
	.param .b64 retval0;
	.loc 3 328 10
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd133, [retval0+0];
	}
	// Callseq End 2
	setp.lt.s32	%p11, %r3, 0;
	.loc 3 328 10
	and.pred  	%p12, %p11, %p10;
	@!%p12 bra 	BB5_17;
	bra.uni 	BB5_13;

BB5_13:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r29}, %fd133;
	}
	xor.b32  	%r30, %r29, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r31, %temp}, %fd133;
	}
	mov.b64 	%fd133, {%r31, %r30};
	bra.uni 	BB5_17;

BB5_14:
	setp.eq.f64	%p13, %fd3, 0d3FF0000000000000;
	.loc 3 328 10
	shr.s32 	%r32, %r8, 31;
	and.b32  	%r33, %r32, -2146435072;
	add.s32 	%r34, %r33, 2146435072;
	setp.lt.s32	%p14, %r3, 0;
	mov.u32 	%r35, 0;
	.loc 3 328 10
	and.pred  	%p15, %p14, %p13;
	or.b32  	%r36, %r34, -2147483648;
	selp.b32	%r37, %r36, %r34, %p15;
	mov.b64 	%fd133, {%r35, %r37};
	bra.uni 	BB5_17;

BB5_15:
	mov.u32 	%r38, 1072693248;
	mov.u32 	%r39, 0;
	.loc 3 328 10
	mov.b64 	%fd133, {%r39, %r38};
	bra.uni 	BB5_17;

BB5_16:
	.loc 3 328 10
	add.f64 	%fd133, %fd132, 0dBFF0000000000000;

BB5_17:
	.loc 1 69 64
	ld.const.u32 	%r40, [%rd33];
	cvt.rn.f64.s32	%fd68, %r40;
	mul.f64 	%fd12, %fd133, %fd68;
	cvt.rn.f64.s32	%fd13, %r72;
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r9}, %fd13;
	}
	mul.f64 	%fd69, %fd13, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd70, %fd69;
	fma.rn.f64 	%fd71, %fd70, 0dC000000000000000, %fd13;
	abs.f64 	%fd14, %fd71;
	setp.eq.s32	%p16, %r7, 0;
	setp.eq.f64	%p17, %fd55, 0d3FF0000000000000;
	.loc 3 328 10
	or.pred  	%p18, %p17, %p16;
	@!%p18 bra 	BB5_19;
	bra.uni 	BB5_18;

BB5_18:
	mov.f64 	%fd134, 0d3FF0000000000000;
	bra.uni 	BB5_33;

BB5_19:
	.loc 3 328 10
	abs.f64 	%fd15, %fd55;
	setp.gtu.f64	%p19, %fd15, 0d7FF0000000000000;
	@%p19 bra 	BB5_32;

	abs.f64 	%fd16, %fd13;
	setp.gtu.f64	%p20, %fd16, 0d7FF0000000000000;
	@%p20 bra 	BB5_32;

	setp.eq.f64	%p21, %fd16, 0d7FF0000000000000;
	@%p21 bra 	BB5_31;

	setp.eq.f64	%p22, %fd15, 0d7FF0000000000000;
	@%p22 bra 	BB5_30;

	setp.eq.f64	%p23, %fd55, 0d0000000000000000;
	.loc 3 328 10
	@%p23 bra 	BB5_29;

	setp.gt.s32	%p24, %r4, -1;
	@%p24 bra 	BB5_27;

	cvt.rzi.f64.f64	%fd72, %fd13;
	setp.eq.f64	%p25, %fd13, %fd72;
	@%p25 bra 	BB5_27;

	mov.f64 	%fd134, 0dFFF8000000000000;
	bra.uni 	BB5_33;

BB5_27:
	setp.eq.f64	%p26, %fd14, 0d3FF0000000000000;
	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd15;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd13;
	.param .b64 retval0;
	.loc 3 328 10
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd134, [retval0+0];
	}
	// Callseq End 3
	setp.lt.s32	%p27, %r4, 0;
	.loc 3 328 10
	and.pred  	%p28, %p27, %p26;
	@!%p28 bra 	BB5_33;
	bra.uni 	BB5_28;

BB5_28:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r41}, %fd134;
	}
	xor.b32  	%r42, %r41, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd134;
	}
	mov.b64 	%fd134, {%r43, %r42};
	bra.uni 	BB5_33;

BB5_29:
	setp.eq.f64	%p29, %fd14, 0d3FF0000000000000;
	.loc 3 328 10
	selp.b32	%r44, %r4, 0, %p29;
	mov.u32 	%r45, 0;
	.loc 3 328 10
	or.b32  	%r46, %r44, 2146435072;
	setp.lt.s32	%p30, %r9, 0;
	selp.b32	%r47, %r46, %r44, %p30;
	mov.b64 	%fd134, {%r45, %r47};
	bra.uni 	BB5_33;

BB5_30:
	setp.eq.f64	%p31, %fd14, 0d3FF0000000000000;
	.loc 3 328 10
	shr.s32 	%r48, %r9, 31;
	and.b32  	%r49, %r48, -2146435072;
	add.s32 	%r50, %r49, 2146435072;
	setp.lt.s32	%p32, %r4, 0;
	mov.u32 	%r51, 0;
	.loc 3 328 10
	and.pred  	%p33, %p32, %p31;
	or.b32  	%r52, %r50, -2147483648;
	selp.b32	%r53, %r52, %r50, %p33;
	mov.b64 	%fd134, {%r51, %r53};
	bra.uni 	BB5_33;

BB5_31:
	setp.eq.f64	%p34, %fd55, 0dBFF0000000000000;
	.loc 3 328 10
	setp.gt.f64	%p35, %fd15, 0d3FF0000000000000;
	selp.b32	%r54, 2146435072, 0, %p35;
	mov.u32 	%r55, 0;
	.loc 3 328 10
	xor.b32  	%r56, %r54, 2146435072;
	setp.lt.s32	%p36, %r9, 0;
	selp.b32	%r57, %r56, %r54, %p36;
	selp.b32	%r58, 1072693248, %r57, %p34;
	mov.b64 	%fd134, {%r55, %r58};
	bra.uni 	BB5_33;

BB5_32:
	.loc 3 328 10
	add.f64 	%fd134, %fd13, %fd55;

BB5_33:
	.loc 1 69 200
	ld.const.u32 	%r59, [%rd32];
	ld.const.u32 	%r60, [%rd30];
	mul.lo.s32 	%r61, %r59, %r60;
	ld.const.u32 	%r62, [%rd31];
	mul.lo.s32 	%r63, %r61, %r62;
	cvt.rn.f64.s32	%fd75, %r63;
	mul.f64 	%fd76, %fd12, %fd134;
	.loc 2 3614 3
	div.rn.f64 	%fd77, %fd76, %fd75;
	.loc 1 78 1
	add.f64 	%fd135, %fd135, %fd77;
	add.f64 	%fd132, %fd132, 0d3FF0000000000000;
	.loc 1 65 1
	add.s64 	%rd33, %rd33, -4;
	add.s64 	%rd32, %rd32, -4;
	add.s64 	%rd31, %rd31, -4;
	add.s32 	%r72, %r72, -2;
	add.s64 	%rd30, %rd30, 4;
	.loc 1 65 72
	add.s32 	%r73, %r73, 1;
	.loc 1 65 1
	setp.le.s32	%p37, %r73, %r2;
	@%p37 bra 	BB5_3;

BB5_34:
	.loc 1 87 1
	cvt.rn.f64.s32	%fd79, %r20;
	mul.f64 	%fd27, %fd79, %fd56;
	.loc 3 203 10
	abs.f64 	%fd28, %fd27;
	setp.neu.f64	%p38, %fd28, 0d7FF0000000000000;
	mov.f64 	%fd140, %fd27;
	@%p38 bra 	BB5_36;

	mov.f64 	%fd80, 0d0000000000000000;
	.loc 3 203 10
	mul.rn.f64 	%fd29, %fd27, %fd80;
	mov.f64 	%fd140, %fd29;

BB5_36:
	.loc 3 203 10
	mov.f64 	%fd30, %fd140;
	add.u64 	%rd16, %SP, 4;
	.loc 3 203 10
	mul.f64 	%fd81, %fd30, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r74, %fd81;
	cvta.to.local.u64 	%rd17, %rd16;
	.loc 3 203 10
	st.local.u32 	[%rd17], %r74;
	cvt.rn.f64.s32	%fd82, %r74;
	neg.f64 	%fd83, %fd82;
	mov.f64 	%fd84, 0d3FF921FB54442D18;
	.loc 3 203 10
	fma.rn.f64 	%fd85, %fd83, %fd84, %fd30;
	mov.f64 	%fd86, 0d3C91A62633145C00;
	.loc 3 203 10
	fma.rn.f64 	%fd87, %fd83, %fd86, %fd85;
	mov.f64 	%fd88, 0d397B839A252049C0;
	.loc 3 203 10
	fma.rn.f64 	%fd136, %fd83, %fd88, %fd87;
	abs.f64 	%fd89, %fd30;
	setp.leu.f64	%p39, %fd89, 0d41E0000000000000;
	@%p39 bra 	BB5_38;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd30;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd16;
	.param .b64 retval0;
	.loc 3 203 10
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd136, [retval0+0];
	}
	// Callseq End 4
	ld.local.u32 	%r74, [%rd17];

BB5_38:
	add.s32 	%r15, %r74, 1;
	shl.b32 	%r64, %r15, 3;
	and.b32  	%r65, %r64, 8;
	and.b32  	%r66, %r15, 1;
	setp.eq.b32	%p40, %r66, 1;
	not.pred 	%p41, %p40;
	selp.f64	%fd90, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p41;
	mul.wide.u32 	%rd20, %r65, 8;
	mov.u64 	%rd21, __cudart_sin_cos_coeffs;
	add.s64 	%rd22, %rd21, %rd20;
	.loc 3 203 10
	ld.const.f64 	%fd91, [%rd22+8];
	mul.rn.f64 	%fd34, %fd136, %fd136;
	fma.rn.f64 	%fd92, %fd90, %fd34, %fd91;
	ld.const.f64 	%fd93, [%rd22+16];
	fma.rn.f64 	%fd94, %fd92, %fd34, %fd93;
	ld.const.f64 	%fd95, [%rd22+24];
	fma.rn.f64 	%fd96, %fd94, %fd34, %fd95;
	ld.const.f64 	%fd97, [%rd22+32];
	fma.rn.f64 	%fd98, %fd96, %fd34, %fd97;
	ld.const.f64 	%fd99, [%rd22+40];
	fma.rn.f64 	%fd100, %fd98, %fd34, %fd99;
	ld.const.f64 	%fd101, [%rd22+48];
	fma.rn.f64 	%fd35, %fd100, %fd34, %fd101;
	fma.rn.f64 	%fd137, %fd35, %fd136, %fd136;
	@%p41 bra 	BB5_40;

	mov.f64 	%fd102, 0d3FF0000000000000;
	.loc 3 203 10
	fma.rn.f64 	%fd137, %fd35, %fd34, %fd102;

BB5_40:
	and.b32  	%r67, %r15, 2;
	setp.eq.s32	%p42, %r67, 0;
	@%p42 bra 	BB5_42;

	mov.f64 	%fd103, 0d0000000000000000;
	mov.f64 	%fd104, 0dBFF0000000000000;
	.loc 3 203 10
	fma.rn.f64 	%fd137, %fd137, %fd104, %fd103;

BB5_42:
	.loc 1 87 75
	mul.f64 	%fd41, %fd135, %fd137;
	neg.f64 	%fd42, %fd135;
	.loc 3 198 10
	mov.f64 	%fd139, %fd27;
	@%p38 bra 	BB5_44;

	mov.f64 	%fd105, 0d0000000000000000;
	.loc 3 198 10
	mul.rn.f64 	%fd139, %fd27, %fd105;

BB5_44:
	add.u64 	%rd23, %SP, 0;
	.loc 3 198 10
	mul.f64 	%fd106, %fd139, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r75, %fd106;
	cvta.to.local.u64 	%rd24, %rd23;
	.loc 3 198 10
	st.local.u32 	[%rd24], %r75;
	cvt.rn.f64.s32	%fd107, %r75;
	neg.f64 	%fd108, %fd107;
	fma.rn.f64 	%fd110, %fd108, %fd84, %fd139;
	fma.rn.f64 	%fd112, %fd108, %fd86, %fd110;
	fma.rn.f64 	%fd141, %fd108, %fd88, %fd112;
	abs.f64 	%fd114, %fd139;
	setp.leu.f64	%p44, %fd114, 0d41E0000000000000;
	@%p44 bra 	BB5_46;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd139;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd23;
	.param .b64 retval0;
	.loc 3 198 10
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd141, [retval0+0];
	}
	// Callseq End 5
	ld.local.u32 	%r75, [%rd24];

BB5_46:
	shl.b32 	%r68, %r75, 3;
	and.b32  	%r69, %r68, 8;
	and.b32  	%r70, %r75, 1;
	setp.eq.b32	%p45, %r70, 1;
	not.pred 	%p46, %p45;
	selp.f64	%fd115, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p46;
	mul.wide.u32 	%rd27, %r69, 8;
	add.s64 	%rd29, %rd21, %rd27;
	.loc 3 198 10
	ld.const.f64 	%fd116, [%rd29+8];
	mul.rn.f64 	%fd48, %fd141, %fd141;
	fma.rn.f64 	%fd117, %fd115, %fd48, %fd116;
	ld.const.f64 	%fd118, [%rd29+16];
	fma.rn.f64 	%fd119, %fd117, %fd48, %fd118;
	ld.const.f64 	%fd120, [%rd29+24];
	fma.rn.f64 	%fd121, %fd119, %fd48, %fd120;
	ld.const.f64 	%fd122, [%rd29+32];
	fma.rn.f64 	%fd123, %fd121, %fd48, %fd122;
	ld.const.f64 	%fd124, [%rd29+40];
	fma.rn.f64 	%fd125, %fd123, %fd48, %fd124;
	ld.const.f64 	%fd126, [%rd29+48];
	fma.rn.f64 	%fd49, %fd125, %fd48, %fd126;
	fma.rn.f64 	%fd142, %fd49, %fd141, %fd141;
	@%p46 bra 	BB5_48;

	mov.f64 	%fd127, 0d3FF0000000000000;
	.loc 3 198 10
	fma.rn.f64 	%fd142, %fd49, %fd48, %fd127;

BB5_48:
	and.b32  	%r71, %r75, 2;
	setp.eq.s32	%p47, %r71, 0;
	@%p47 bra 	BB5_50;

	mov.f64 	%fd128, 0d0000000000000000;
	mov.f64 	%fd129, 0dBFF0000000000000;
	.loc 3 198 10
	fma.rn.f64 	%fd142, %fd142, %fd129, %fd128;

BB5_50:
	.loc 1 87 191
	mul.f64 	%fd130, %fd42, %fd142;
	st.param.f64	[func_retval0+0], %fd41;
	st.param.f64	[func_retval0+8], %fd130;
	ret;
}

.visible .entry _Z27computeZernikeFeatureVectoriiiPhiiPd(
	.param .u32 _Z27computeZernikeFeatureVectoriiiPhiiPd_param_0,
	.param .u32 _Z27computeZernikeFeatureVectoriiiPhiiPd_param_1,
	.param .u32 _Z27computeZernikeFeatureVectoriiiPhiiPd_param_2,
	.param .u64 _Z27computeZernikeFeatureVectoriiiPhiiPd_param_3,
	.param .u32 _Z27computeZernikeFeatureVectoriiiPhiiPd_param_4,
	.param .u32 _Z27computeZernikeFeatureVectoriiiPhiiPd_param_5,
	.param .u64 _Z27computeZernikeFeatureVectoriiiPhiiPd_param_6
)
{
	.local .align 8 .b8 	__local_depot6[120];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<84>;
	.reg .s16 	%rs<2>;
	.reg .s32 	%r<212>;
	.reg .s64 	%rd<63>;
	.reg .f64 	%fd<266>;


	mov.u64 	%SPL, __local_depot6;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r60, [_Z27computeZernikeFeatureVectoriiiPhiiPd_param_0];
	ld.param.u32 	%r64, [_Z27computeZernikeFeatureVectoriiiPhiiPd_param_1];
	ld.param.u32 	%r61, [_Z27computeZernikeFeatureVectoriiiPhiiPd_param_2];
	ld.param.u64 	%rd22, [_Z27computeZernikeFeatureVectoriiiPhiiPd_param_3];
	ld.param.u32 	%r62, [_Z27computeZernikeFeatureVectoriiiPhiiPd_param_4];
	ld.param.u32 	%r63, [_Z27computeZernikeFeatureVectoriiiPhiiPd_param_5];
	ld.param.u64 	%rd21, [_Z27computeZernikeFeatureVectoriiiPhiiPd_param_6];
	cvta.to.global.u64 	%rd1, %rd21;
	cvta.to.global.u64 	%rd2, %rd22;
	add.u64 	%rd23, %SP, 0;
	cvta.to.local.u64 	%rd3, %rd23;
	add.u64 	%rd24, %SP, 4;
	cvta.to.local.u64 	%rd4, %rd24;
	add.u64 	%rd25, %SPL, 8;
	.loc 1 95 1
	mov.u32 	%r65, %ntid.x;
	mov.u32 	%r66, %ctaid.x;
	mov.u32 	%r67, %tid.x;
	mad.lo.s32 	%r1, %r65, %r66, %r67;
	.loc 1 96 1
	mov.u32 	%r68, %ntid.y;
	mov.u32 	%r69, %ctaid.y;
	mov.u32 	%r70, %tid.y;
	mad.lo.s32 	%r2, %r68, %r69, %r70;
	.loc 1 97 1
	sub.s32 	%r71, %r64, %r60;
	setp.gt.s32	%p1, %r1, %r71;
	sub.s32 	%r72, %r61, %r60;
	setp.gt.s32	%p2, %r2, %r72;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	BB6_92;

	.loc 1 101 1
	mad.lo.s32 	%r3, %r1, %r61, %r2;
	mov.u32 	%r73, 1;
	.loc 1 102 1
	sub.s32 	%r74, %r73, %r60;
	add.s32 	%r75, %r74, %r61;
	mad.lo.s32 	%r76, %r1, %r75, %r2;
	mul.lo.s32 	%r77, %r76, 14;
	mul.wide.s32 	%rd26, %r77, 8;
	add.s64 	%rd6, %rd1, %rd26;
	.loc 1 105 1
	setp.lt.s32	%p4, %r62, 1;
	@%p4 bra 	BB6_10;

	mov.u32 	%r195, 255;
	.loc 1 34 1
	setp.gt.s32	%p5, %r60, 0;
	@%p5 bra 	BB6_4;

	mov.u32 	%r191, 0;
	bra.uni 	BB6_8;

BB6_4:
	mov.u32 	%r81, 0;
	mov.u32 	%r186, %r81;
	mov.u32 	%r194, %r81;

BB6_5:
	.loc 1 36 1
	mov.u32 	%r190, %r194;
	mov.u32 	%r192, %r190;
	add.s32 	%r7, %r186, %r3;
	mov.u32 	%r193, %r81;

BB6_6:
	.loc 1 36 1
	mov.u32 	%r8, %r193;
	mad.lo.s32 	%r84, %r8, %r61, %r7;
	cvt.s64.s32	%rd27, %r84;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.u8 	%r85, [%rd28];
	max.s32 	%r192, %r85, %r192;
	min.s32 	%r195, %r85, %r195;
	.loc 1 35 101
	add.s32 	%r13, %r8, 1;
	.loc 1 35 1
	setp.lt.s32	%p6, %r13, %r60;
	mov.u32 	%r193, %r13;
	@%p6 bra 	BB6_6;

	.loc 1 34 101
	add.s32 	%r186, %r186, 1;
	.loc 1 34 1
	setp.lt.s32	%p7, %r186, %r60;
	mov.u32 	%r191, %r192;
	mov.u32 	%r194, %r192;
	@%p7 bra 	BB6_5;

BB6_8:
	.loc 1 44 1
	sub.s32 	%r86, %r191, %r195;
	.loc 1 105 32
	setp.gt.s32	%p8, %r86, %r62;
	@%p8 bra 	BB6_10;

	mov.u64 	%rd29, -4616189618054758400;
	.loc 1 108 1
	st.global.u64 	[%rd6+96], %rd29;
	.loc 1 109 1
	st.global.u64 	[%rd6+104], %rd29;
	bra.uni 	BB6_92;

BB6_10:
	.loc 1 113 1
	setp.lt.s32	%p9, %r63, 1;
	@%p9 bra 	BB6_19;

	setp.gt.s32	%p10, %r60, 0;
	.loc 1 20 1
	@%p10 bra 	BB6_13;

	mov.f64 	%fd251, 0d0000000000000000;
	bra.uni 	BB6_17;

BB6_13:
	mov.u32 	%r87, 0;
	mov.f64 	%fd251, 0d0000000000000000;
	mov.u32 	%r198, %r87;

BB6_14:
	.loc 1 22 1
	add.s32 	%r18, %r198, %r3;
	mov.u32 	%r197, %r87;

BB6_15:
	.loc 1 22 1
	mov.u32 	%r19, %r197;
	mad.lo.s32 	%r89, %r19, %r61, %r18;
	cvt.s64.s32	%rd30, %r89;
	add.s64 	%rd31, %rd2, %rd30;
	ld.global.u8 	%r90, [%rd31];
	cvt.rn.f64.s32	%fd85, %r90;
	add.f64 	%fd251, %fd251, %fd85;
	.loc 1 21 101
	add.s32 	%r20, %r19, 1;
	.loc 1 21 1
	setp.lt.s32	%p11, %r20, %r60;
	mov.u32 	%r197, %r20;
	@%p11 bra 	BB6_15;

	.loc 1 20 101
	add.s32 	%r198, %r198, 1;
	.loc 1 20 1
	setp.lt.s32	%p12, %r198, %r60;
	@%p12 bra 	BB6_14;

BB6_17:
	.loc 1 25 1
	mul.lo.s32 	%r91, %r60, %r60;
	cvt.rn.f64.s32	%fd86, %r91;
	.loc 2 3614 3
	div.rn.f64 	%fd87, %fd251, %fd86;
	.loc 1 113 40
	cvt.rn.f64.s32	%fd88, %r63;
	setp.ltu.f64	%p13, %fd87, %fd88;
	@%p13 bra 	BB6_19;

	mov.u64 	%rd32, -4616189618054758400;
	.loc 1 116 1
	st.global.u64 	[%rd6+96], %rd32;
	.loc 1 117 1
	st.global.u64 	[%rd6+104], %rd32;
	bra.uni 	BB6_92;

BB6_19:
	.loc 1 129 1
	add.s32 	%r95, %r60, -1;
	cvt.rn.f64.s32	%fd89, %r95;
	.loc 2 3614 3
	mul.f64 	%fd5, %fd89, 0d3FE0000000000000;
	mov.u32 	%r205, 0;
	mov.u32 	%r199, %r205;
	mov.u64 	%rd62, factorials;
	mov.u32 	%r208, %r205;

BB6_20:
	.loc 1 65 1
	mov.u64 	%rd60, %rd62;
	mov.u64 	%rd7, %rd60;
	mov.u32 	%r206, %r208;
	mov.u32 	%r22, %r206;
	mov.u32 	%r203, %r205;
	mov.u32 	%r24, %r203;
	.loc 1 132 1
	setp.lt.s32	%p14, %r24, 0;
	@%p14 bra 	BB6_90;

	.loc 1 131 93
	add.s32 	%r97, %r24, 1;
	.loc 1 158 1
	cvt.rn.f64.s32	%fd90, %r97;
	.loc 2 3614 3
	div.rn.f64 	%fd6, %fd90, 0d400921FB54442D18;
	mov.u32 	%r200, 0;

BB6_22:
	.loc 1 133 1
	sub.s32 	%r27, %r24, %r200;
	shr.u32 	%r98, %r27, 31;
	add.s32 	%r99, %r27, %r98;
	and.b32  	%r100, %r99, -2;
	sub.s32 	%r101, %r27, %r100;
	setp.eq.s32	%p15, %r101, 1;
	@%p15 bra 	BB6_89;

	setp.gt.s32	%p16, %r60, 0;
	.loc 1 138 1
	@%p16 bra 	BB6_25;

	mov.f64 	%fd265, 0d0000000000000000;
	mov.f64 	%fd264, %fd265;
	bra.uni 	BB6_88;

BB6_25:
	.loc 1 87 1
	cvt.rn.f64.s32	%fd7, %r200;
	.loc 1 69 200
	add.s32 	%r103, %r200, %r24;
	shr.u32 	%r104, %r103, 31;
	add.s32 	%r105, %r103, %r104;
	shr.s32 	%r106, %r105, 1;
	.loc 1 65 1
	mul.wide.s32 	%rd34, %r106, 4;
	mov.u64 	%rd35, factorials;
	add.s64 	%rd8, %rd35, %rd34;
	shr.s32 	%r29, %r99, 1;
	mul.wide.s32 	%rd36, %r29, 4;
	add.s64 	%rd9, %rd35, %rd36;
	mov.u32 	%r201, 0;
	mov.f64 	%fd265, 0d0000000000000000;
	mov.f64 	%fd264, %fd265;

BB6_26:
	mov.u32 	%r202, 0;
	.loc 1 141 1
	cvt.rn.f64.s32	%fd95, %r201;
	sub.f64 	%fd10, %fd95, %fd5;
	.loc 1 48 1
	mul.f64 	%fd11, %fd10, %fd10;

BB6_27:
	.loc 1 141 1
	cvt.rn.f64.s32	%fd96, %r202;
	sub.f64 	%fd14, %fd96, %fd5;
	.loc 1 48 1
	fma.rn.f64 	%fd97, %fd14, %fd14, %fd11;
	.loc 2 3060 10
	sqrt.rn.f64 	%fd15, %fd97;
	.loc 1 52 1
	setp.eq.f64	%p17, %fd14, 0d0000000000000000;
	setp.eq.f64	%p18, %fd10, 0d0000000000000000;
	and.pred  	%p19, %p17, %p18;
	.loc 1 52 1
	@!%p19 bra 	BB6_29;
	bra.uni 	BB6_28;

BB6_28:
	mov.f64 	%fd252, 0d0000000000000000;
	bra.uni 	BB6_34;

BB6_29:
	.loc 3 278 10
	abs.f64 	%fd16, %fd10;
	setp.eq.f64	%p20, %fd16, 0d0000000000000000;
	abs.f64 	%fd17, %fd14;
	setp.eq.f64	%p21, %fd17, 0d0000000000000000;
	and.pred  	%p22, %p20, %p21;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r33}, %fd10;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r110}, %fd14;
	}
	and.b32  	%r34, %r110, -2147483648;
	@%p22 bra 	BB6_33;

	setp.eq.f64	%p23, %fd16, 0d7FF0000000000000;
	setp.eq.f64	%p24, %fd17, 0d7FF0000000000000;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	BB6_32;

	setp.lt.s32	%p26, %r33, 0;
	.loc 3 278 10
	min.f64 	%fd98, %fd17, %fd16;
	max.f64 	%fd99, %fd17, %fd16;
	div.rn.f64 	%fd100, %fd98, %fd99;
	mul.f64 	%fd101, %fd100, %fd100;
	mov.f64 	%fd102, 0d3F2D3B63DBB65B49;
	mov.f64 	%fd103, 0dBEF53E1D2A25FF7E;
	.loc 3 278 10
	fma.rn.f64 	%fd104, %fd103, %fd101, %fd102;
	mov.f64 	%fd105, 0dBF5312788DDE082E;
	.loc 3 278 10
	fma.rn.f64 	%fd106, %fd104, %fd101, %fd105;
	mov.f64 	%fd107, 0d3F6F9690C8249315;
	.loc 3 278 10
	fma.rn.f64 	%fd108, %fd106, %fd101, %fd107;
	mov.f64 	%fd109, 0dBF82CF5AABC7CF0D;
	.loc 3 278 10
	fma.rn.f64 	%fd110, %fd108, %fd101, %fd109;
	mov.f64 	%fd111, 0d3F9162B0B2A3BFDE;
	.loc 3 278 10
	fma.rn.f64 	%fd112, %fd110, %fd101, %fd111;
	mov.f64 	%fd113, 0dBF9A7256FEB6FC6B;
	.loc 3 278 10
	fma.rn.f64 	%fd114, %fd112, %fd101, %fd113;
	mov.f64 	%fd115, 0d3FA171560CE4A489;
	.loc 3 278 10
	fma.rn.f64 	%fd116, %fd114, %fd101, %fd115;
	mov.f64 	%fd117, 0dBFA4F44D841450E4;
	.loc 3 278 10
	fma.rn.f64 	%fd118, %fd116, %fd101, %fd117;
	mov.f64 	%fd119, 0d3FA7EE3D3F36BB95;
	.loc 3 278 10
	fma.rn.f64 	%fd120, %fd118, %fd101, %fd119;
	mov.f64 	%fd121, 0dBFAAD32AE04A9FD1;
	.loc 3 278 10
	fma.rn.f64 	%fd122, %fd120, %fd101, %fd121;
	mov.f64 	%fd123, 0d3FAE17813D66954F;
	.loc 3 278 10
	fma.rn.f64 	%fd124, %fd122, %fd101, %fd123;
	mov.f64 	%fd125, 0dBFB11089CA9A5BCD;
	.loc 3 278 10
	fma.rn.f64 	%fd126, %fd124, %fd101, %fd125;
	mov.f64 	%fd127, 0d3FB3B12B2DB51738;
	.loc 3 278 10
	fma.rn.f64 	%fd128, %fd126, %fd101, %fd127;
	mov.f64 	%fd129, 0dBFB745D022F8DC5C;
	.loc 3 278 10
	fma.rn.f64 	%fd130, %fd128, %fd101, %fd129;
	mov.f64 	%fd131, 0d3FBC71C709DFE927;
	.loc 3 278 10
	fma.rn.f64 	%fd132, %fd130, %fd101, %fd131;
	mov.f64 	%fd133, 0dBFC2492491FA1744;
	.loc 3 278 10
	fma.rn.f64 	%fd134, %fd132, %fd101, %fd133;
	mov.f64 	%fd135, 0d3FC99999999840D2;
	.loc 3 278 10
	fma.rn.f64 	%fd136, %fd134, %fd101, %fd135;
	mov.f64 	%fd137, 0dBFD555555555544C;
	.loc 3 278 10
	fma.rn.f64 	%fd138, %fd136, %fd101, %fd137;
	mul.f64 	%fd139, %fd138, %fd101;
	fma.rn.f64 	%fd140, %fd139, %fd100, %fd100;
	mov.f64 	%fd141, 0d3FF921FB54442D18;
	.loc 3 278 10
	sub.f64 	%fd142, %fd141, %fd140;
	setp.gt.f64	%p27, %fd17, %fd16;
	selp.f64	%fd143, %fd142, %fd140, %p27;
	mov.f64 	%fd144, 0d400921FB54442D18;
	.loc 3 278 10
	sub.f64 	%fd145, %fd144, %fd143;
	selp.f64	%fd146, %fd145, %fd143, %p26;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r111, %temp}, %fd146;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r112}, %fd146;
	}
	or.b32  	%r113, %r112, %r34;
	mov.b64 	%fd147, {%r111, %r113};
	add.f64 	%fd148, %fd16, %fd17;
	setp.gtu.f64	%p28, %fd148, 0d7FF0000000000000;
	selp.f64	%fd252, %fd148, %fd147, %p28;
	bra.uni 	BB6_34;

BB6_32:
	setp.lt.s32	%p29, %r33, 0;
	.loc 3 278 10
	selp.f64	%fd149, 0d4002D97C7F3321D2, 0d3FE921FB54442D18, %p29;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r114, %temp}, %fd149;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r115}, %fd149;
	}
	or.b32  	%r116, %r115, %r34;
	mov.b64 	%fd252, {%r114, %r116};
	bra.uni 	BB6_34;

BB6_33:
	setp.lt.s32	%p30, %r33, 0;
	.loc 3 278 10
	selp.f64	%fd150, 0d400921FB54442D18, 0d0000000000000000, %p30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r117, %temp}, %fd150;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r118}, %fd150;
	}
	or.b32  	%r119, %r118, %r34;
	mov.b64 	%fd252, {%r117, %r119};

BB6_34:
	.loc 2 3614 3
	div.rn.f64 	%fd22, %fd15, %fd5;
	.loc 1 146 1
	setp.gt.f64	%p31, %fd22, 0d3FF0000000000000;
	@%p31 bra 	BB6_86;

	.loc 1 133 1
	sub.s32 	%r178, %r24, %r200;
	.loc 1 61 1
	and.b32  	%r177, %r178, 1;
	setp.lt.s32	%p32, %r178, -1;
	.loc 1 61 1
	setp.ne.s32	%p33, %r177, 0;
	or.pred  	%p34, %p33, %p32;
	@!%p34 bra 	BB6_37;
	bra.uni 	BB6_36;

BB6_36:
	mov.f64 	%fd256, 0d0000000000000000;
	bra.uni 	BB6_69;

BB6_37:
	.loc 1 65 1
	mov.u64 	%rd57, factorials;
	mov.f64 	%fd154, 0dBFF0000000000000;
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd154;
	}
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd22;
	}
	mov.f64 	%fd256, 0d0000000000000000;
	mov.u32 	%r209, 0;
	mov.f64 	%fd253, %fd256;
	mov.u64 	%rd59, %rd8;
	mov.u64 	%rd58, %rd9;
	mov.u32 	%r204, %r24;
	mov.u32 	%r207, %r22;
	mov.u64 	%rd61, %rd7;

BB6_38:
	.loc 1 131 1
	mov.u64 	%rd15, %rd61;
	mov.u32 	%r38, %r207;
	.loc 1 65 1
	mov.u32 	%r37, %r204;
	.loc 3 328 10
	mul.f64 	%fd155, %fd253, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd156, %fd155;
	fma.rn.f64 	%fd157, %fd156, 0dC000000000000000, %fd253;
	abs.f64 	%fd25, %fd157;
	setp.ne.s32	%p35, %r209, 0;
	@%p35 bra 	BB6_40;

	mov.f64 	%fd254, 0d3FF0000000000000;
	bra.uni 	BB6_52;

BB6_40:
	mov.f64 	%fd244, 0dBFF0000000000000;
	.loc 3 328 10
	abs.f64 	%fd26, %fd244;
	setp.gtu.f64	%p36, %fd26, 0d7FF0000000000000;
	@%p36 bra 	BB6_51;

	abs.f64 	%fd27, %fd253;
	setp.gtu.f64	%p37, %fd27, 0d7FF0000000000000;
	@%p37 bra 	BB6_51;

	setp.eq.f64	%p38, %fd27, 0d7FF0000000000000;
	@%p38 bra 	BB6_50;

	setp.eq.f64	%p39, %fd26, 0d7FF0000000000000;
	@%p39 bra 	BB6_49;

	setp.gt.s32	%p40, %r35, -1;
	@%p40 bra 	BB6_47;

	cvt.rzi.f64.f64	%fd159, %fd253;
	setp.eq.f64	%p41, %fd253, %fd159;
	@%p41 bra 	BB6_47;

	mov.f64 	%fd254, 0dFFF8000000000000;
	bra.uni 	BB6_52;

BB6_47:
	setp.eq.f64	%p42, %fd25, 0d3FF0000000000000;
	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd26;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd253;
	.param .b64 retval0;
	.loc 3 328 10
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd254, [retval0+0];
	}
	// Callseq End 6
	setp.lt.s32	%p43, %r35, 0;
	.loc 3 328 10
	and.pred  	%p44, %p43, %p42;
	@!%p44 bra 	BB6_52;
	bra.uni 	BB6_48;

BB6_48:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r121}, %fd254;
	}
	xor.b32  	%r122, %r121, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r123, %temp}, %fd254;
	}
	mov.b64 	%fd254, {%r123, %r122};
	bra.uni 	BB6_52;

BB6_49:
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r182}, %fd253;
	}
	setp.eq.f64	%p45, %fd25, 0d3FF0000000000000;
	.loc 3 328 10
	shr.s32 	%r124, %r182, 31;
	and.b32  	%r125, %r124, -2146435072;
	add.s32 	%r126, %r125, 2146435072;
	setp.lt.s32	%p46, %r35, 0;
	mov.u32 	%r127, 0;
	.loc 3 328 10
	and.pred  	%p47, %p46, %p45;
	or.b32  	%r128, %r126, -2147483648;
	selp.b32	%r129, %r128, %r126, %p47;
	mov.b64 	%fd254, {%r127, %r129};
	bra.uni 	BB6_52;

BB6_50:
	mov.u32 	%r130, 1072693248;
	mov.u32 	%r131, 0;
	.loc 3 328 10
	mov.b64 	%fd254, {%r131, %r130};
	bra.uni 	BB6_52;

BB6_51:
	.loc 3 328 10
	add.f64 	%fd254, %fd253, 0dBFF0000000000000;

BB6_52:
	.loc 1 69 64
	ld.const.u32 	%r132, [%rd15];
	cvt.rn.f64.s32	%fd162, %r132;
	mul.f64 	%fd34, %fd254, %fd162;
	cvt.rn.f64.s32	%fd35, %r37;
	.loc 3 328 10
	mul.f64 	%fd163, %fd35, 0d3FE0000000000000;
	cvt.rzi.f64.f64	%fd164, %fd163;
	fma.rn.f64 	%fd165, %fd164, 0dC000000000000000, %fd35;
	abs.f64 	%fd36, %fd165;
	setp.eq.s32	%p48, %r38, 0;
	setp.eq.f64	%p49, %fd22, 0d3FF0000000000000;
	.loc 3 328 10
	or.pred  	%p50, %p49, %p48;
	@!%p50 bra 	BB6_54;
	bra.uni 	BB6_53;

BB6_53:
	mov.f64 	%fd255, 0d3FF0000000000000;
	bra.uni 	BB6_68;

BB6_54:
	.loc 3 328 10
	abs.f64 	%fd37, %fd22;
	setp.gtu.f64	%p51, %fd37, 0d7FF0000000000000;
	@%p51 bra 	BB6_67;

	abs.f64 	%fd38, %fd35;
	setp.gtu.f64	%p52, %fd38, 0d7FF0000000000000;
	@%p52 bra 	BB6_67;

	setp.eq.f64	%p53, %fd38, 0d7FF0000000000000;
	@%p53 bra 	BB6_66;

	abs.f64 	%fd245, %fd22;
	setp.eq.f64	%p54, %fd245, 0d7FF0000000000000;
	@%p54 bra 	BB6_65;

	setp.eq.f64	%p55, %fd22, 0d0000000000000000;
	.loc 3 328 10
	@%p55 bra 	BB6_64;

	setp.gt.s32	%p56, %r36, -1;
	@%p56 bra 	BB6_62;

	cvt.rzi.f64.f64	%fd166, %fd35;
	setp.eq.f64	%p57, %fd35, %fd166;
	@%p57 bra 	BB6_62;

	mov.f64 	%fd255, 0dFFF8000000000000;
	bra.uni 	BB6_68;

BB6_62:
	.loc 3 328 10
	abs.f64 	%fd246, %fd22;
	setp.eq.f64	%p58, %fd36, 0d3FF0000000000000;
	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd246;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd35;
	.param .b64 retval0;
	.loc 3 328 10
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd255, [retval0+0];
	}
	// Callseq End 7
	setp.lt.s32	%p59, %r36, 0;
	.loc 3 328 10
	and.pred  	%p60, %p59, %p58;
	@!%p60 bra 	BB6_68;
	bra.uni 	BB6_63;

BB6_63:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r133}, %fd255;
	}
	xor.b32  	%r134, %r133, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r135, %temp}, %fd255;
	}
	mov.b64 	%fd255, {%r135, %r134};
	bra.uni 	BB6_68;

BB6_64:
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r183}, %fd35;
	}
	setp.eq.f64	%p61, %fd36, 0d3FF0000000000000;
	.loc 3 328 10
	selp.b32	%r136, %r36, 0, %p61;
	mov.u32 	%r137, 0;
	.loc 3 328 10
	or.b32  	%r138, %r136, 2146435072;
	setp.lt.s32	%p62, %r183, 0;
	selp.b32	%r139, %r138, %r136, %p62;
	mov.b64 	%fd255, {%r137, %r139};
	bra.uni 	BB6_68;

BB6_65:
	.loc 3 328 10
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r184}, %fd35;
	}
	setp.eq.f64	%p63, %fd36, 0d3FF0000000000000;
	.loc 3 328 10
	shr.s32 	%r140, %r184, 31;
	and.b32  	%r141, %r140, -2146435072;
	add.s32 	%r142, %r141, 2146435072;
	setp.lt.s32	%p64, %r36, 0;
	mov.u32 	%r143, 0;
	.loc 3 328 10
	and.pred  	%p65, %p64, %p63;
	or.b32  	%r144, %r142, -2147483648;
	selp.b32	%r145, %r144, %r142, %p65;
	mov.b64 	%fd255, {%r143, %r145};
	bra.uni 	BB6_68;

BB6_66:
	.loc 3 328 10
	abs.f64 	%fd247, %fd22;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r185}, %fd35;
	}
	setp.eq.f64	%p66, %fd22, 0dBFF0000000000000;
	.loc 3 328 10
	setp.gt.f64	%p67, %fd247, 0d3FF0000000000000;
	selp.b32	%r146, 2146435072, 0, %p67;
	mov.u32 	%r147, 0;
	.loc 3 328 10
	xor.b32  	%r148, %r146, 2146435072;
	setp.lt.s32	%p68, %r185, 0;
	selp.b32	%r149, %r148, %r146, %p68;
	selp.b32	%r150, 1072693248, %r149, %p66;
	mov.b64 	%fd255, {%r147, %r150};
	bra.uni 	BB6_68;

BB6_67:
	.loc 3 328 10
	add.f64 	%fd255, %fd22, %fd35;

BB6_68:
	.loc 1 69 200
	ld.const.u32 	%r151, [%rd59];
	ld.const.u32 	%r152, [%rd57];
	mul.lo.s32 	%r153, %r151, %r152;
	ld.const.u32 	%r154, [%rd58];
	mul.lo.s32 	%r155, %r153, %r154;
	cvt.rn.f64.s32	%fd169, %r155;
	mul.f64 	%fd170, %fd34, %fd255;
	.loc 2 3614 3
	div.rn.f64 	%fd171, %fd170, %fd169;
	.loc 1 78 1
	add.f64 	%fd256, %fd256, %fd171;
	add.f64 	%fd253, %fd253, 0d3FF0000000000000;
	.loc 1 65 1
	add.s64 	%rd16, %rd15, -4;
	add.s64 	%rd59, %rd59, -4;
	add.s64 	%rd58, %rd58, -4;
	add.s32 	%r42, %r38, 2;
	add.s32 	%r43, %r37, -2;
	add.s64 	%rd57, %rd57, 4;
	.loc 1 65 72
	add.s32 	%r209, %r209, 1;
	.loc 1 65 1
	setp.le.s32	%p69, %r209, %r29;
	mov.u32 	%r204, %r43;
	mov.u32 	%r207, %r42;
	mov.u64 	%rd61, %rd16;
	@%p69 bra 	BB6_38;

BB6_69:
	.loc 1 87 1
	mul.f64 	%fd49, %fd7, %fd252;
	.loc 3 203 10
	abs.f64 	%fd50, %fd49;
	setp.neu.f64	%p70, %fd50, 0d7FF0000000000000;
	mov.f64 	%fd261, %fd49;
	@%p70 bra 	BB6_71;

	mov.f64 	%fd173, 0d0000000000000000;
	.loc 3 203 10
	mul.rn.f64 	%fd51, %fd49, %fd173;
	mov.f64 	%fd261, %fd51;

BB6_71:
	.loc 3 203 10
	mov.f64 	%fd52, %fd261;
	mul.f64 	%fd174, %fd52, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r210, %fd174;
	st.local.u32 	[%rd4], %r210;
	cvt.rn.f64.s32	%fd175, %r210;
	neg.f64 	%fd176, %fd175;
	mov.f64 	%fd177, 0d3FF921FB54442D18;
	.loc 3 203 10
	fma.rn.f64 	%fd178, %fd176, %fd177, %fd52;
	mov.f64 	%fd179, 0d3C91A62633145C00;
	.loc 3 203 10
	fma.rn.f64 	%fd180, %fd176, %fd179, %fd178;
	mov.f64 	%fd181, 0d397B839A252049C0;
	.loc 3 203 10
	fma.rn.f64 	%fd257, %fd176, %fd181, %fd180;
	abs.f64 	%fd182, %fd52;
	setp.leu.f64	%p71, %fd182, 0d41E0000000000000;
	@%p71 bra 	BB6_73;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd52;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd24;
	.param .b64 retval0;
	.loc 3 203 10
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd257, [retval0+0];
	}
	// Callseq End 8
	ld.local.u32 	%r210, [%rd4];

BB6_73:
	add.s32 	%r48, %r210, 1;
	shl.b32 	%r156, %r48, 3;
	and.b32  	%r157, %r156, 8;
	and.b32  	%r158, %r48, 1;
	setp.eq.b32	%p72, %r158, 1;
	not.pred 	%p73, %p72;
	selp.f64	%fd183, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p73;
	mul.wide.u32 	%rd39, %r157, 8;
	mov.u64 	%rd40, __cudart_sin_cos_coeffs;
	add.s64 	%rd41, %rd40, %rd39;
	.loc 3 203 10
	ld.const.f64 	%fd184, [%rd41+8];
	mul.rn.f64 	%fd56, %fd257, %fd257;
	fma.rn.f64 	%fd185, %fd183, %fd56, %fd184;
	ld.const.f64 	%fd186, [%rd41+16];
	fma.rn.f64 	%fd187, %fd185, %fd56, %fd186;
	ld.const.f64 	%fd188, [%rd41+24];
	fma.rn.f64 	%fd189, %fd187, %fd56, %fd188;
	ld.const.f64 	%fd190, [%rd41+32];
	fma.rn.f64 	%fd191, %fd189, %fd56, %fd190;
	ld.const.f64 	%fd192, [%rd41+40];
	fma.rn.f64 	%fd193, %fd191, %fd56, %fd192;
	ld.const.f64 	%fd194, [%rd41+48];
	fma.rn.f64 	%fd57, %fd193, %fd56, %fd194;
	fma.rn.f64 	%fd258, %fd57, %fd257, %fd257;
	@%p73 bra 	BB6_75;

	mov.f64 	%fd195, 0d3FF0000000000000;
	.loc 3 203 10
	fma.rn.f64 	%fd258, %fd57, %fd56, %fd195;

BB6_75:
	and.b32  	%r159, %r48, 2;
	setp.eq.s32	%p74, %r159, 0;
	@%p74 bra 	BB6_77;

	mov.f64 	%fd196, 0d0000000000000000;
	mov.f64 	%fd197, 0dBFF0000000000000;
	.loc 3 203 10
	fma.rn.f64 	%fd258, %fd258, %fd197, %fd196;

BB6_77:
	.loc 1 87 75
	mul.f64 	%fd63, %fd256, %fd258;
	neg.f64 	%fd64, %fd256;
	.loc 3 198 10
	mov.f64 	%fd260, %fd49;
	@%p70 bra 	BB6_79;

	mov.f64 	%fd198, 0d0000000000000000;
	.loc 3 198 10
	mul.rn.f64 	%fd260, %fd49, %fd198;

BB6_79:
	mov.f64 	%fd250, 0d397B839A252049C0;
	mov.f64 	%fd249, 0d3C91A62633145C00;
	mov.f64 	%fd248, 0d3FF921FB54442D18;
	.loc 3 198 10
	mul.f64 	%fd199, %fd260, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r211, %fd199;
	st.local.u32 	[%rd3], %r211;
	cvt.rn.f64.s32	%fd200, %r211;
	neg.f64 	%fd201, %fd200;
	fma.rn.f64 	%fd203, %fd201, %fd248, %fd260;
	fma.rn.f64 	%fd205, %fd201, %fd249, %fd203;
	fma.rn.f64 	%fd262, %fd201, %fd250, %fd205;
	abs.f64 	%fd207, %fd260;
	setp.leu.f64	%p76, %fd207, 0d41E0000000000000;
	@%p76 bra 	BB6_81;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.f64	[param0+0], %fd260;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd23;
	.param .b64 retval0;
	.loc 3 198 10
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd262, [retval0+0];
	}
	// Callseq End 9
	ld.local.u32 	%r211, [%rd3];

BB6_81:
	mov.u64 	%rd56, __cudart_sin_cos_coeffs;
	.loc 3 198 10
	shl.b32 	%r160, %r211, 3;
	and.b32  	%r161, %r160, 8;
	and.b32  	%r162, %r211, 1;
	setp.eq.b32	%p77, %r162, 1;
	not.pred 	%p78, %p77;
	selp.f64	%fd208, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p78;
	mul.wide.u32 	%rd43, %r161, 8;
	add.s64 	%rd45, %rd56, %rd43;
	.loc 3 198 10
	ld.const.f64 	%fd209, [%rd45+8];
	mul.rn.f64 	%fd70, %fd262, %fd262;
	fma.rn.f64 	%fd210, %fd208, %fd70, %fd209;
	ld.const.f64 	%fd211, [%rd45+16];
	fma.rn.f64 	%fd212, %fd210, %fd70, %fd211;
	ld.const.f64 	%fd213, [%rd45+24];
	fma.rn.f64 	%fd214, %fd212, %fd70, %fd213;
	ld.const.f64 	%fd215, [%rd45+32];
	fma.rn.f64 	%fd216, %fd214, %fd70, %fd215;
	ld.const.f64 	%fd217, [%rd45+40];
	fma.rn.f64 	%fd218, %fd216, %fd70, %fd217;
	ld.const.f64 	%fd219, [%rd45+48];
	fma.rn.f64 	%fd71, %fd218, %fd70, %fd219;
	fma.rn.f64 	%fd263, %fd71, %fd262, %fd262;
	@%p78 bra 	BB6_83;

	mov.f64 	%fd220, 0d3FF0000000000000;
	.loc 3 198 10
	fma.rn.f64 	%fd263, %fd71, %fd70, %fd220;

BB6_83:
	and.b32  	%r163, %r211, 2;
	setp.eq.s32	%p79, %r163, 0;
	@%p79 bra 	BB6_85;

	mov.f64 	%fd221, 0d0000000000000000;
	mov.f64 	%fd222, 0dBFF0000000000000;
	.loc 3 198 10
	fma.rn.f64 	%fd263, %fd263, %fd222, %fd221;

BB6_85:
	.loc 1 151 1
	add.s32 	%r179, %r201, %r3;
	mad.lo.s32 	%r164, %r202, %r61, %r179;
	cvt.s64.s32	%rd46, %r164;
	add.s64 	%rd47, %rd2, %rd46;
	ld.global.u8 	%rs1, [%rd47];
	cvt.rn.f64.u16	%fd223, %rs1;
	fma.rn.f64 	%fd265, %fd63, %fd223, %fd265;
	.loc 1 87 191
	mul.f64 	%fd224, %fd64, %fd263;
	.loc 1 153 1
	fma.rn.f64 	%fd264, %fd224, %fd223, %fd264;

BB6_86:
	.loc 1 139 101
	add.s32 	%r202, %r202, 1;
	.loc 1 139 1
	setp.lt.s32	%p80, %r202, %r60;
	@%p80 bra 	BB6_27;

	.loc 1 138 101
	add.s32 	%r201, %r201, 1;
	.loc 1 138 1
	setp.lt.s32	%p81, %r201, %r60;
	@%p81 bra 	BB6_26;

BB6_88:
	.loc 1 158 61
	mul.f64 	%fd225, %fd265, %fd6;
	.loc 1 159 60
	mul.f64 	%fd226, %fd264, %fd6;
	.loc 1 161 1
	mul.f64 	%fd227, %fd226, %fd226;
	fma.rn.f64 	%fd228, %fd225, %fd225, %fd227;
	.loc 2 3060 10
	sqrt.rn.f64 	%fd229, %fd228;
	.loc 1 161 105
	mul.wide.s32 	%rd48, %r199, 8;
	add.s64 	%rd49, %rd25, %rd48;
	st.local.f64 	[%rd49], %fd229;
	.loc 1 163 1
	add.s32 	%r199, %r199, 1;

BB6_89:
	.loc 1 132 128
	add.s32 	%r200, %r200, 1;
	.loc 1 132 1
	setp.le.s32	%p82, %r200, %r24;
	@%p82 bra 	BB6_22;

BB6_90:
	.loc 1 131 93
	add.s32 	%r205, %r24, 1;
	.loc 1 131 1
	setp.lt.s32	%p83, %r205, 6;
	add.s64 	%rd62, %rd7, 4;
	add.s32 	%r208, %r22, -1;
	@%p83 bra 	BB6_20;

	ld.param.u64 	%rd55, [_Z27computeZernikeFeatureVectoriiiPhiiPd_param_6];
	cvta.to.global.u64 	%rd54, %rd55;
	.loc 1 166 1
	add.s32 	%r172, %r61, 1;
	sub.s32 	%r173, %r172, %r60;
	mad.lo.s32 	%r175, %r1, %r173, %r2;
	mul.lo.s32 	%r176, %r175, 14;
	mul.wide.s32 	%rd51, %r176, 8;
	add.s64 	%rd52, %rd54, %rd51;
	.loc 1 167 1
	ld.local.f64 	%fd230, [%rd25];
	ld.local.f64 	%fd231, [%rd25+8];
	st.global.f64 	[%rd52], %fd230;
	st.global.f64 	[%rd52+8], %fd231;
	ld.local.f64 	%fd232, [%rd25+16];
	ld.local.f64 	%fd233, [%rd25+24];
	st.global.f64 	[%rd52+16], %fd232;
	st.global.f64 	[%rd52+24], %fd233;
	ld.local.f64 	%fd234, [%rd25+32];
	ld.local.f64 	%fd235, [%rd25+40];
	st.global.f64 	[%rd52+32], %fd234;
	st.global.f64 	[%rd52+40], %fd235;
	ld.local.f64 	%fd236, [%rd25+48];
	ld.local.f64 	%fd237, [%rd25+56];
	st.global.f64 	[%rd52+48], %fd236;
	st.global.f64 	[%rd52+56], %fd237;
	ld.local.f64 	%fd238, [%rd25+64];
	ld.local.f64 	%fd239, [%rd25+72];
	st.global.f64 	[%rd52+64], %fd238;
	st.global.f64 	[%rd52+72], %fd239;
	ld.local.f64 	%fd240, [%rd25+80];
	ld.local.f64 	%fd241, [%rd25+88];
	st.global.f64 	[%rd52+80], %fd240;
	st.global.f64 	[%rd52+88], %fd241;
	.loc 1 169 1
	cvt.rn.f64.s32	%fd242, %r1;
	st.global.f64 	[%rd52+96], %fd242;
	.loc 1 170 1
	cvt.rn.f64.s32	%fd243, %r2;
	st.global.f64 	[%rd52+104], %fd243;

BB6_92:
	.loc 1 171 2
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot7[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<8>;
	.reg .s32 	%r<48>;
	.reg .s64 	%rd<90>;
	.reg .f64 	%fd<3>;


	mov.u64 	%SPL, __local_depot7;
	ld.param.f64 	%fd1, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd30, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd31, %SPL, 0;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd1;
	}
	and.b32  	%r46, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	and.b32  	%r16, %r3, 2047;
	add.s32 	%r17, %r16, -1024;
	mov.b64 	 %rd32, %fd1;
	shl.b64 	%rd33, %rd32, 11;
	or.b64  	%rd2, %rd33, -9223372036854775808;
	shr.u32 	%r18, %r17, 6;
	mov.u32 	%r19, 16;
	sub.s32 	%r4, %r19, %r18;
	mov.u32 	%r20, 15;
	sub.s32 	%r45, %r20, %r18;
	mov.u32 	%r21, 19;
	sub.s32 	%r22, %r21, %r18;
	mov.u32 	%r23, 18;
	min.s32 	%r6, %r23, %r22;
	setp.lt.s32	%p1, %r45, %r6;
	@%p1 bra 	BB7_2;

	mov.u64 	%rd84, 0;
	bra.uni 	BB7_4;

BB7_2:
	sub.s32 	%r29, %r20, %r18;
	mul.wide.s32 	%rd36, %r29, 8;
	mov.u64 	%rd37, __cudart_i2opi_d;
	add.s64 	%rd83, %rd37, %rd36;
	mov.u64 	%rd84, 0;
	mov.u64 	%rd82, %rd31;

BB7_3:
	.pragma "nounroll";
	mov.u64 	%rd4, %rd82;
	ld.const.u64 	%rd40, [%rd83];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd40;    
	mov.b64         {blo,bhi}, %rd2;    
	mov.b64         {clo,chi}, %rd84;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd38, {r0,r1};      
	mov.b64         %rd39, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd4], %rd38;
	add.s64 	%rd83, %rd83, 8;
	add.s64 	%rd9, %rd4, 8;
	add.s32 	%r45, %r45, 1;
	setp.lt.s32	%p2, %r45, %r6;
	mov.u64 	%rd84, %rd39;
	mov.u64 	%rd82, %rd9;
	@%p2 bra 	BB7_3;

BB7_4:
	mov.u32 	%r30, 1;
	sub.s32 	%r31, %r30, %r4;
	add.s32 	%r32, %r31, %r45;
	mul.wide.s32 	%rd43, %r32, 8;
	add.s64 	%rd44, %rd31, %rd43;
	st.local.u64 	[%rd44], %rd84;
	ld.local.u64 	%rd85, [%rd31+16];
	ld.local.u64 	%rd86, [%rd31+24];
	and.b32  	%r10, %r3, 63;
	setp.eq.s32	%p3, %r10, 0;
	@%p3 bra 	BB7_6;

	mov.u32 	%r33, 64;
	sub.s32 	%r34, %r33, %r10;
	shl.b64 	%rd45, %rd86, %r10;
	shr.u64 	%rd46, %rd85, %r34;
	or.b64  	%rd86, %rd45, %rd46;
	shl.b64 	%rd47, %rd85, %r10;
	ld.local.u64 	%rd48, [%rd31+8];
	shr.u64 	%rd49, %rd48, %r34;
	or.b64  	%rd85, %rd49, %rd47;

BB7_6:
	shr.u64 	%rd50, %rd86, 62;
	cvt.u32.u64	%r35, %rd50;
	shr.u64 	%rd51, %rd85, 62;
	shl.b64 	%rd52, %rd86, 2;
	or.b64  	%rd88, %rd52, %rd51;
	shl.b64 	%rd87, %rd85, 2;
	shr.u64 	%rd53, %rd86, 61;
	cvt.u32.u64	%r36, %rd53;
	and.b32  	%r37, %r36, 1;
	add.s32 	%r38, %r37, %r35;
	neg.s32 	%r39, %r38;
	setp.eq.s32	%p4, %r46, 0;
	selp.b32	%r40, %r38, %r39, %p4;
	st.u32 	[%rd30], %r40;
	setp.eq.s32	%p5, %r37, 0;
	@%p5 bra 	BB7_8;

	mov.u64 	%rd57, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd57;
	mov.b64         {a2,a3}, %rd57;
	mov.b64         {b0,b1}, %rd87;
	mov.b64         {b2,b3}, %rd88;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd54, {r0,r1};
	mov.b64         %rd55, {r2,r3};
	}
	// inline asm
	xor.b32  	%r46, %r46, -2147483648;
	mov.u64 	%rd88, %rd55;
	mov.u64 	%rd87, %rd54;

BB7_8:
	clz.b64 	%r47, %rd88;
	setp.eq.s32	%p6, %r47, 0;
	@%p6 bra 	BB7_10;

	shl.b64 	%rd60, %rd88, %r47;
	mov.u32 	%r41, 64;
	sub.s32 	%r42, %r41, %r47;
	shr.u64 	%rd61, %rd87, %r42;
	or.b64  	%rd88, %rd61, %rd60;

BB7_10:
	mov.u64 	%rd65, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd88;   
	mov.b64         {blo,bhi}, %rd65;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd62, {r0,r1};     
	mov.b64         %rd63, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p7, %rd63, 1;
	mov.u64 	%rd89, %rd63;
	@%p7 bra 	BB7_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd62;
	mov.b64         {a2,a3}, %rd63;
	mov.b64         {b0,b1}, %rd62;
	mov.b64         {b2,b3}, %rd63;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd66, {r0,r1};
	mov.b64         %rd67, {r2,r3};
	}
	// inline asm
	add.s32 	%r47, %r47, 1;
	mov.u64 	%rd89, %rd67;

BB7_12:
	cvt.u64.u32	%rd72, %r46;
	shl.b64 	%rd73, %rd72, 32;
	mov.u32 	%r43, 1022;
	sub.s32 	%r44, %r43, %r47;
	cvt.u64.u32	%rd74, %r44;
	shl.b64 	%rd75, %rd74, 52;
	add.s64 	%rd76, %rd89, 1;
	shr.u64 	%rd77, %rd76, 10;
	add.s64 	%rd78, %rd77, 1;
	shr.u64 	%rd79, %rd78, 1;
	add.s64 	%rd80, %rd75, %rd79;
	or.b64  	%rd81, %rd80, %rd73;
	mov.b64 	 %fd2, %rd81;
	st.param.f64	[func_retval0+0], %fd2;
	ret;
}

.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<11>;
	.reg .s32 	%r<39>;
	.reg .f32 	%f<5>;
	.reg .f64 	%fd<144>;


	ld.param.f64 	%fd14, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd15, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd14;
	}
	shr.u32 	%r18, %r35, 20;
	and.b32  	%r36, %r18, 2047;
	setp.ne.s32	%p1, %r36, 0;
	@%p1 bra 	BB8_2;

	mul.f64 	%fd16, %fd14, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd16;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd16;
	}
	shr.u32 	%r19, %r35, 20;
	and.b32  	%r20, %r19, 2047;
	add.s32 	%r36, %r20, -54;

BB8_2:
	add.s32 	%r37, %r36, -1023;
	and.b32  	%r21, %r35, -2146435073;
	or.b32  	%r22, %r21, 1072693248;
	mov.b64 	%fd141, {%r34, %r22};
	setp.lt.u32	%p2, %r22, 1073127583;
	@%p2 bra 	BB8_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd141;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd141;
	}
	add.s32 	%r25, %r24, -1048576;
	mov.b64 	%fd141, {%r23, %r25};
	add.s32 	%r37, %r36, -1022;

BB8_4:
	add.f64 	%fd17, %fd141, 0d3FF0000000000000;
	mov.f64 	%fd19, 0d3FF0000000000000;
	// inline asm
	cvt.rn.f32.f64     %f1,%fd17;
	// inline asm
	// inline asm
	rcp.approx.ftz.f32 %f2,%f1;
	// inline asm
	// inline asm
	cvt.f64.f32        %fd18,%f2;
	// inline asm
	neg.f64 	%fd20, %fd17;
	fma.rn.f64 	%fd21, %fd20, %fd18, %fd19;
	fma.rn.f64 	%fd22, %fd21, %fd21, %fd21;
	fma.rn.f64 	%fd23, %fd22, %fd18, %fd18;
	add.f64 	%fd24, %fd141, 0dBFF0000000000000;
	mul.f64 	%fd25, %fd24, %fd23;
	fma.rn.f64 	%fd26, %fd24, %fd23, %fd25;
	mul.f64 	%fd27, %fd26, %fd26;
	mov.f64 	%fd28, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd29, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd30, %fd29, %fd27, %fd28;
	mov.f64 	%fd31, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd32, %fd30, %fd27, %fd31;
	mov.f64 	%fd33, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd34, %fd32, %fd27, %fd33;
	mov.f64 	%fd35, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd36, %fd34, %fd27, %fd35;
	mov.f64 	%fd37, 0d3F6249249242B910;
	fma.rn.f64 	%fd38, %fd36, %fd27, %fd37;
	mov.f64 	%fd39, 0d3F89999999999DFB;
	fma.rn.f64 	%fd40, %fd38, %fd27, %fd39;
	sub.f64 	%fd41, %fd24, %fd26;
	add.f64 	%fd42, %fd41, %fd41;
	neg.f64 	%fd43, %fd26;
	fma.rn.f64 	%fd44, %fd43, %fd24, %fd42;
	mul.f64 	%fd45, %fd23, %fd44;
	fma.rn.f64 	%fd46, %fd40, %fd27, 0d3FB5555555555555;
	mov.f64 	%fd47, 0d3FB5555555555555;
	sub.f64 	%fd48, %fd47, %fd46;
	fma.rn.f64 	%fd49, %fd40, %fd27, %fd48;
	add.f64 	%fd50, %fd49, 0d0000000000000000;
	add.f64 	%fd51, %fd50, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd52, %fd46, %fd51;
	sub.f64 	%fd53, %fd46, %fd52;
	add.f64 	%fd54, %fd53, %fd51;
	mul.rn.f64 	%fd55, %fd52, %fd26;
	neg.f64 	%fd56, %fd55;
	fma.rn.f64 	%fd57, %fd52, %fd26, %fd56;
	fma.rn.f64 	%fd58, %fd52, %fd45, %fd57;
	fma.rn.f64 	%fd59, %fd54, %fd26, %fd58;
	add.f64 	%fd60, %fd55, %fd59;
	sub.f64 	%fd61, %fd55, %fd60;
	add.f64 	%fd62, %fd61, %fd59;
	mul.rn.f64 	%fd63, %fd60, %fd26;
	neg.f64 	%fd64, %fd63;
	fma.rn.f64 	%fd65, %fd60, %fd26, %fd64;
	fma.rn.f64 	%fd66, %fd60, %fd45, %fd65;
	fma.rn.f64 	%fd67, %fd62, %fd26, %fd66;
	add.f64 	%fd68, %fd63, %fd67;
	sub.f64 	%fd69, %fd63, %fd68;
	add.f64 	%fd70, %fd69, %fd67;
	mul.rn.f64 	%fd71, %fd68, %fd26;
	neg.f64 	%fd72, %fd71;
	fma.rn.f64 	%fd73, %fd68, %fd26, %fd72;
	fma.rn.f64 	%fd74, %fd68, %fd45, %fd73;
	fma.rn.f64 	%fd75, %fd70, %fd26, %fd74;
	add.f64 	%fd76, %fd71, %fd75;
	sub.f64 	%fd77, %fd71, %fd76;
	add.f64 	%fd78, %fd77, %fd75;
	add.f64 	%fd79, %fd26, %fd76;
	sub.f64 	%fd80, %fd26, %fd79;
	add.f64 	%fd81, %fd80, %fd76;
	add.f64 	%fd82, %fd81, %fd78;
	add.f64 	%fd83, %fd82, %fd45;
	add.f64 	%fd84, %fd79, %fd83;
	sub.f64 	%fd85, %fd79, %fd84;
	add.f64 	%fd86, %fd85, %fd83;
	cvt.rn.f64.s32	%fd87, %r37;
	mov.f64 	%fd88, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd89, %fd87, %fd88;
	mov.f64 	%fd90, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd91, %fd87, %fd90;
	add.f64 	%fd92, %fd89, %fd84;
	sub.f64 	%fd93, %fd89, %fd92;
	add.f64 	%fd94, %fd93, %fd84;
	add.f64 	%fd95, %fd94, %fd86;
	add.f64 	%fd96, %fd95, %fd91;
	add.f64 	%fd97, %fd92, %fd96;
	sub.f64 	%fd98, %fd92, %fd97;
	add.f64 	%fd99, %fd98, %fd96;
	abs.f64 	%fd100, %fd15;
	setp.gt.f64	%p3, %fd100, 0d7F0D2A1BE4048F90;
	mul.f64 	%fd101, %fd15, 0d3F20000000000000;
	selp.f64	%fd102, %fd101, %fd15, %p3;
	mul.rn.f64 	%fd103, %fd97, %fd102;
	neg.f64 	%fd104, %fd103;
	fma.rn.f64 	%fd105, %fd97, %fd102, %fd104;
	fma.rn.f64 	%fd106, %fd99, %fd102, %fd105;
	add.f64 	%fd4, %fd103, %fd106;
	sub.f64 	%fd107, %fd103, %fd4;
	add.f64 	%fd5, %fd107, %fd106;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd4;
	}
	setp.lt.u32	%p4, %r13, 1082535491;
	setp.lt.s32	%p5, %r13, -1064875759;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	BB8_6;

	setp.lt.s32	%p7, %r13, 0;
	selp.f64	%fd108, 0d0000000000000000, 0d7FF0000000000000, %p7;
	abs.f64 	%fd109, %fd4;
	setp.gtu.f64	%p8, %fd109, 0d7FF0000000000000;
	add.f64 	%fd110, %fd4, %fd4;
	selp.f64	%fd143, %fd110, %fd108, %p8;
	bra.uni 	BB8_10;

BB8_6:
	mul.f64 	%fd111, %fd4, 0d3FF71547652B82FE;
	cvt.rni.f64.f64	%fd112, %fd111;
	cvt.rzi.s32.f64	%r14, %fd112;
	mov.f64 	%fd113, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd114, %fd112, %fd113, %fd4;
	mov.f64 	%fd115, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd116, %fd112, %fd115, %fd114;
	mov.f64 	%fd117, 0d3E928A27E30F5561;
	mov.f64 	%fd118, 0d3E5AE6449C0686C0;
	fma.rn.f64 	%fd119, %fd118, %fd116, %fd117;
	mov.f64 	%fd120, 0d3EC71DE8E6486D6B;
	fma.rn.f64 	%fd121, %fd119, %fd116, %fd120;
	mov.f64 	%fd122, 0d3EFA019A6B2464C5;
	fma.rn.f64 	%fd123, %fd121, %fd116, %fd122;
	mov.f64 	%fd124, 0d3F2A01A0171064A5;
	fma.rn.f64 	%fd125, %fd123, %fd116, %fd124;
	mov.f64 	%fd126, 0d3F56C16C17F29C8D;
	fma.rn.f64 	%fd127, %fd125, %fd116, %fd126;
	mov.f64 	%fd128, 0d3F8111111111A24E;
	fma.rn.f64 	%fd129, %fd127, %fd116, %fd128;
	mov.f64 	%fd130, 0d3FA555555555211D;
	fma.rn.f64 	%fd131, %fd129, %fd116, %fd130;
	mov.f64 	%fd132, 0d3FC5555555555530;
	fma.rn.f64 	%fd133, %fd131, %fd116, %fd132;
	mov.f64 	%fd134, 0d3FE0000000000005;
	fma.rn.f64 	%fd135, %fd133, %fd116, %fd134;
	fma.rn.f64 	%fd137, %fd135, %fd116, %fd19;
	fma.rn.f64 	%fd142, %fd137, %fd116, %fd19;
	abs.s32 	%r26, %r14;
	setp.lt.s32	%p9, %r26, 1023;
	@%p9 bra 	BB8_8;

	add.s32 	%r27, %r14, 2046;
	shl.b32 	%r28, %r27, 19;
	and.b32  	%r29, %r28, -1048576;
	shl.b32 	%r30, %r27, 20;
	sub.s32 	%r38, %r30, %r29;
	mov.u32 	%r31, 0;
	mov.b64 	%fd138, {%r31, %r29};
	mul.f64 	%fd142, %fd142, %fd138;
	bra.uni 	BB8_9;

BB8_8:
	shl.b32 	%r32, %r14, 20;
	add.s32 	%r38, %r32, 1072693248;

BB8_9:
	mov.u32 	%r33, 0;
	mov.b64 	%fd139, {%r33, %r38};
	mul.f64 	%fd143, %fd142, %fd139;

BB8_10:
	abs.f64 	%fd140, %fd143;
	setp.eq.f64	%p10, %fd140, 0d7FF0000000000000;
	@%p10 bra 	BB8_12;

	fma.rn.f64 	%fd143, %fd143, %fd5, %fd143;

BB8_12:
	st.param.f64	[func_retval0+0], %fd143;
	ret;
}


